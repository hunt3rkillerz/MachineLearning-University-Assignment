{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut \n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "housingData = pd.read_table('property_prices.csv', \",\")\n",
    "\n",
    "#Removing noisy elements of dataset\n",
    "housingData = housingData[(housingData[['price_bands']] != \"0K-200K\").all(axis=1)]\n",
    "housingData = housingData[(housingData[['price_bands']] != \"Unknown\").all(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns that are independentant, or not useful\n",
    "X = housingData\n",
    "\n",
    "X = X.drop(columns=\"id\")\n",
    "X = X.drop(columns=\"lattitude\")\n",
    "X = X.drop(columns=\"longtitude\")\n",
    "X = X.drop(columns=\"address\")\n",
    "X = X.drop(columns=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreProcess Data\n",
    "#Let's prepare the data with just replacing the missing data with the median\n",
    "\n",
    "\n",
    "im_median = sklearn.preprocessing.Imputer(strategy='median', axis=0)\n",
    "X[['rooms', 'bathrooms', 'car_parks', 'bedrooms', 'landsize', 'building_area', 'year_built']] = im_median.fit_transform(\n",
    "    X[['rooms', 'bathrooms', 'car_parks', 'bedrooms', 'landsize', 'building_area', 'year_built']])\n",
    "\n",
    "\n",
    "\n",
    "#Remove all rooms that are 0\n",
    "\n",
    "X = X[(X[['rooms', 'bathrooms', 'bedrooms']] != 0).all(axis=1)]\n",
    "Y = X['price_bands']\n",
    "X = X.drop(columns='price_bands')\n",
    "\n",
    "for col in X.columns:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = SelectKBest(k=15)\n",
    "select.fit(X, Y)\n",
    "newFeat = []\n",
    "toRemove = []\n",
    "mask = select.get_support(indices=True)\n",
    "featNames = list(X.columns.values)\n",
    "newX = X\n",
    "for bool, feat in zip(mask, featNames):\n",
    "    if bool:\n",
    "        newX[feat] = X[feat]\n",
    "        newFeat.append(feat)\n",
    "            \n",
    "newX = X[newFeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      1M-2M       0.57      0.56      0.57      8982\n",
      "  200K-400K       0.45      0.46      0.45      1082\n",
      "     2M-20M       0.40      0.41      0.41      1955\n",
      "  400K-600K       0.47      0.45      0.46      4452\n",
      "  600K-800K       0.41      0.41      0.41      6053\n",
      "    800K-1M       0.25      0.27      0.26      4664\n",
      "\n",
      "avg / total       0.45      0.45      0.45     27188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Base Decision Tree\n",
    "model = sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "prediction = cross_val_predict(model, newX, Y, cv=5)\n",
    "\n",
    "print(metrics.classification_report(Y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      1M-2M       0.58      0.57      0.57      8982\n",
      "  200K-400K       0.45      0.44      0.45      1082\n",
      "     2M-20M       0.40      0.40      0.40      1955\n",
      "  400K-600K       0.47      0.46      0.46      4452\n",
      "  600K-800K       0.40      0.40      0.40      6053\n",
      "    800K-1M       0.26      0.27      0.26      4664\n",
      "\n",
      "avg / total       0.45      0.45      0.45     27188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pre Prunded Decision Tree\n",
    "model = sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=20)\n",
    "prediction = cross_val_predict(model, newX, Y, cv=5)\n",
    "\n",
    "print(metrics.classification_report(Y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      1M-2M       0.62      0.68      0.65      8982\n",
      "  200K-400K       0.61      0.43      0.50      1082\n",
      "     2M-20M       0.52      0.46      0.49      1955\n",
      "  400K-600K       0.53      0.53      0.53      4452\n",
      "  600K-800K       0.46      0.48      0.47      6053\n",
      "    800K-1M       0.30      0.26      0.28      4664\n",
      "\n",
      "avg / total       0.51      0.51      0.51     27188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ensemble Bagging Decision Tree (Also pre pruned)\n",
    "model = sklearn.ensemble.BaggingClassifier(sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=20), max_samples=1.0, max_features=0.9, n_estimators = 50)\n",
    "\n",
    "prediction = cross_val_predict(model, newX, Y, cv=5)\n",
    "\n",
    "print(metrics.classification_report(Y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      1M-2M       0.60      0.65      0.63      8982\n",
      "  200K-400K       0.55      0.33      0.41      1082\n",
      "     2M-20M       0.35      0.43      0.38      1955\n",
      "  400K-600K       0.51      0.49      0.50      4452\n",
      "  600K-800K       0.46      0.48      0.47      6053\n",
      "    800K-1M       0.29      0.24      0.26      4664\n",
      "\n",
      "avg / total       0.48      0.49      0.48     27188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ensemble AdaBoost Decision Tree (Also pre pruned)\n",
    "model = sklearn.ensemble.AdaBoostClassifier(sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=20),\n",
    "                                            n_estimators = 50)\n",
    "prediction = cross_val_predict(model, newX, Y, cv=5)\n",
    "\n",
    "print(metrics.classification_report(Y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      1M-2M       0.62      0.76      0.69      8982\n",
      "  200K-400K       0.65      0.42      0.51      1082\n",
      "     2M-20M       0.63      0.45      0.52      1955\n",
      "  400K-600K       0.56      0.55      0.55      4452\n",
      "  600K-800K       0.49      0.53      0.51      6053\n",
      "    800K-1M       0.33      0.22      0.27      4664\n",
      "\n",
      "avg / total       0.53      0.55      0.53     27188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Ensemble Classifier\n",
    "model = sklearn.ensemble.RandomForestClassifier(criterion=\"entropy\", n_estimators = 50, min_samples_split=25, \n",
    "                                                                                  random_state = 0)\n",
    "\n",
    "prediction = cross_val_predict(model, newX, Y, cv=5)\n",
    "\n",
    "print(metrics.classification_report(Y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      1M-2M       0.62      0.77      0.69      8982\n",
      "  200K-400K       0.66      0.43      0.52      1082\n",
      "     2M-20M       0.62      0.44      0.52      1955\n",
      "  400K-600K       0.56      0.55      0.55      4452\n",
      "  600K-800K       0.49      0.53      0.51      6053\n",
      "    800K-1M       0.33      0.22      0.26      4664\n",
      "\n",
      "avg / total       0.53      0.55      0.53     27188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Ensemble Classifier with Pre-Pruning\n",
    "model = sklearn.ensemble.RandomForestClassifier(criterion=\"entropy\", n_estimators = 50, max_depth=20, min_samples_split=25, \n",
    "                                                                                  random_state = 0)\n",
    "prediction = cross_val_predict(model, newX, Y, cv=5)\n",
    "\n",
    "print(metrics.classification_report(Y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      1M-2M       0.63      0.79      0.70      8982\n",
      "  200K-400K       0.67      0.41      0.51      1082\n",
      "     2M-20M       0.67      0.43      0.52      1955\n",
      "  400K-600K       0.56      0.54      0.55      4452\n",
      "  600K-800K       0.50      0.57      0.53      6053\n",
      "    800K-1M       0.35      0.20      0.26      4664\n",
      "\n",
      "avg / total       0.54      0.56      0.54     27188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ensemble Bagging Random Forest (with Pre-Pruning)\n",
    "model = sklearn.ensemble.BaggingClassifier(sklearn.ensemble.RandomForestClassifier(criterion=\"entropy\", n_estimators = 20,\n",
    "                                                                                 max_depth=50, min_samples_split=25, \n",
    "                                                                                  random_state = 0), n_estimators = 50)\n",
    "prediction = cross_val_predict(model, newX, Y, cv=5)\n",
    "\n",
    "print(metrics.classification_report(Y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      1M-2M       0.59      0.70      0.64      8982\n",
      "  200K-400K       0.63      0.25      0.36      1082\n",
      "     2M-20M       0.47      0.10      0.16      1955\n",
      "  400K-600K       0.51      0.48      0.49      4452\n",
      "  600K-800K       0.43      0.55      0.48      6053\n",
      "    800K-1M       0.28      0.23      0.25      4664\n",
      "\n",
      "avg / total       0.48      0.49      0.47     27188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ensemble AdaBoost Random Forest\n",
    "model = sklearn.ensemble.AdaBoostClassifier(sklearn.ensemble.RandomForestClassifier(criterion=\"entropy\", n_estimators = 10,\n",
    "                                                                                 max_depth=50, min_samples_split=25, \n",
    "                                                                                  random_state = 0), n_estimators = 50)\n",
    "prediction = cross_val_predict(model, newX, Y, cv=5)\n",
    "\n",
    "print(metrics.classification_report(Y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
